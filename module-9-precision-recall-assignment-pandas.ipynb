{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring precision and recall\n",
    "\n",
    "The goal of this second notebook is to understand precision-recall in the context of classifiers.\n",
    "\n",
    " * Use Amazon review data in its entirety.\n",
    " * Train a logistic regression model.\n",
    " * Explore various evaluation metrics: accuracy, confusion matrix, precision, recall.\n",
    " * Explore how various metrics can be combined to produce a cost of making an error.\n",
    " * Explore precision and recall curves.\n",
    " \n",
    "Because we are using the full Amazon review dataset (not a subset of words or reviews), in this assignment we return to using GraphLab Create for its efficiency. As usual, let's start by **firing up GraphLab Create**.\n",
    "\n",
    "Make sure you have the latest version of GraphLab Create (1.8.3 or later). If you don't find the decision tree module, then you would need to upgrade graphlab-create using\n",
    "\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```\n",
    "See [this page](https://dato.com/download/) for detailed instructions on upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import graphlab\n",
    "import sframe\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "# graphlab.canvas.set_target('ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load amazon review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-05-25 21:43:15,594 [INFO] sframe.cython.cy_server, 172: SFrame v1.9 started. Logging /tmp/sframe_server_1464226995.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 105953 lines. Lines per second: 109517</pre>"
      ],
      "text/plain": [
       "Read 105953 lines. Lines per second: 109517"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/asifiqbal/Development/courses/Machine Learning - Classification/amazon_baby.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/asifiqbal/Development/courses/Machine Learning - Classification/amazon_baby.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 183531 lines in 1.37569 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 183531 lines in 1.37569 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[str,str,int]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "products = sframe.SFrame.read_csv('amazon_baby.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract word counts and sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the first assignment of this course, we compute the word counts for individual words and extract positive and negative sentiments from ratings. To summarize, we perform the following:\n",
    "\n",
    "1. Remove punctuation.\n",
    "2. Remove reviews with \"neutral\" sentiment (rating 3).\n",
    "3. Set reviews with rating 4 or more to be positive and those with 2 or less to be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "# Remove punctuation.\n",
    "review_clean = products['review'].apply(remove_punctuation)\n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "\n",
    "# Count words\n",
    "#  products['word_count'] = graphlab.text_analytics.count_words(review_clean)\n",
    "\n",
    "# Drop neutral sentiment reviews.\n",
    "products = products[products['rating'] != 3]\n",
    "\n",
    "# Positive sentiment to +1 and negative sentiment to -1\n",
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remember what the dataset looks like by taking a quick peek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">name</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">rating</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_clean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Planetwise Wipe Pouch</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">it came early and was not<br>disappointed. i love ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">it came early and was not<br>disappointed i love ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Annas Dream Full Quilt<br>with 2 Shams ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Very soft and comfortable<br>and warmer than it ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Very soft and comfortable<br>and warmer than it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This is a product well<br>worth the purchase.  I ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This is a product well<br>worth the purchase  I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All of my kids have cried<br>non-stop when I tried to ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All of my kids have cried<br>nonstop when I tried to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Stop Pacifier Sucking<br>without tears with ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">When the Binky Fairy came<br>to our house, we didn't ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">When the Binky Fairy came<br>to our house we didnt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A Tale of Baby's Days<br>with Peter Rabbit ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Lovely book, it's bound<br>tightly so you may no ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Lovely book its bound<br>tightly so you may no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Perfect for new parents.<br>We were able to keep ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Perfect for new parents<br>We were able to keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A friend of mine pinned<br>this product on Pinte ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A friend of mine pinned<br>this product on Pinte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This has been an easy way<br>for my nanny to record ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This has been an easy way<br>for my nanny to record ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Baby Tracker&amp;reg; - Daily<br>Childcare Journal, ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I love this journal and<br>our nanny uses it ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I love this journal and<br>our nanny uses it ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[166752 rows x 5 columns]<br/>Note: Only the head of the SFrame is printed.<br/>You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tname\tstr\n",
       "\treview\tstr\n",
       "\trating\tint\n",
       "\tsentiment\tint\n",
       "\treview_clean\tstr\n",
       "\n",
       "Rows: 166752\n",
       "\n",
       "Data:\n",
       "+-------------------------------+-------------------------------+--------+-----------+\n",
       "|              name             |             review            | rating | sentiment |\n",
       "+-------------------------------+-------------------------------+--------+-----------+\n",
       "|     Planetwise Wipe Pouch     | it came early and was not ... |   5    |     1     |\n",
       "| Annas Dream Full Quilt wit... | Very soft and comfortable ... |   5    |     1     |\n",
       "| Stop Pacifier Sucking with... | This is a product well wor... |   5    |     1     |\n",
       "| Stop Pacifier Sucking with... | All of my kids have cried ... |   5    |     1     |\n",
       "| Stop Pacifier Sucking with... | When the Binky Fairy came ... |   5    |     1     |\n",
       "| A Tale of Baby's Days with... | Lovely book, it's bound ti... |   4    |     1     |\n",
       "| Baby Tracker&reg; - Daily ... | Perfect for new parents. W... |   5    |     1     |\n",
       "| Baby Tracker&reg; - Daily ... | A friend of mine pinned th... |   5    |     1     |\n",
       "| Baby Tracker&reg; - Daily ... | This has been an easy way ... |   4    |     1     |\n",
       "| Baby Tracker&reg; - Daily ... | I love this journal and ou... |   4    |     1     |\n",
       "+-------------------------------+-------------------------------+--------+-----------+\n",
       "+-------------------------------+\n",
       "|          review_clean         |\n",
       "+-------------------------------+\n",
       "| it came early and was not ... |\n",
       "| Very soft and comfortable ... |\n",
       "| This is a product well wor... |\n",
       "| All of my kids have cried ... |\n",
       "| When the Binky Fairy came ... |\n",
       "| Lovely book its bound tigh... |\n",
       "| Perfect for new parents We... |\n",
       "| A friend of mine pinned th... |\n",
       "| This has been an easy way ... |\n",
       "| I love this journal and ou... |\n",
       "+-------------------------------+\n",
       "[166752 rows x 5 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "\n",
    "We split the data into a 80-20 split where 80% is in the training set and 20% is in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a logistic regression classifier\n",
    "\n",
    "We will now train a logistic regression classifier with **sentiment** as the target and **word_count** as the features. We will set `validation_set=None` to make sure everyone gets exactly the same results.  \n",
    "\n",
    "Remember, even though we now know how to implement logistic regression, we will use GraphLab Create for its efficiency at processing this Amazon dataset in its entirety.  The focus of this assignment is instead on the topic of precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "     # Use this token pattern to keep single-letter words\n",
    "# First, learn vocabulary from the training data and assign columns to words\n",
    "# Then convert the training data into a sparse matrix\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "model = linear_model.LogisticRegression()\n",
    "model.fit(train_matrix, train_data['sentiment'])\n",
    "\n",
    "# model = graphlab.logistic_classifier.create(train_data, target='sentiment',\n",
    "#                                             features=['word_count'],\n",
    "#                                             validation_set=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will explore the advanced model evaluation concepts that were discussed in the lectures.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "One performance metric we will use for our more advanced exploration is accuracy, which we have seen many times in past assignments.  Recall that the accuracy is given by\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$\n",
    "\n",
    "To obtain the accuracy of our trained models using GraphLab Create, simply pass the option `metric='accuracy'` to the `evaluate` function. We compute the **accuracy** of our logistic regression model on the **test_data** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.932295416367\n"
     ]
    }
   ],
   "source": [
    "# accuracy= model.evaluate(test_data, metric='accuracy')['accuracy']\n",
    "# print \"Test Accuracy: %s\" % accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true=test_data['sentiment'].to_numpy(), y_pred=model.predict(test_matrix))\n",
    "print \"Test Accuracy: %s\" % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "Recall from an earlier assignment that we used the **majority class classifier** as a baseline (i.e reference) model for a point of comparison with a more sophisticated classifier. The majority classifier model predicts the majority class for all data points. \n",
    "\n",
    "Typically, a good model should beat the majority class classifier. Since the majority class in this dataset is the positive class (i.e., there are more positive than negative reviews), the accuracy of the majority class classifier can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy (majority class classifier): 0.842782577394\n"
     ]
    }
   ],
   "source": [
    "baseline = len(test_data[test_data['sentiment'] == 1])/len(test_data)\n",
    "print \"Baseline accuracy (majority class classifier): %s\" % baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** Using accuracy as the evaluation metric, was our **logistic regression model** better than the baseline (majority class classifier)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "The accuracy, while convenient, does not tell the whole story. For a fuller picture, we turn to the **confusion matrix**. In the case of binary classification, the confusion matrix is a 2-by-2 matrix laying out correct and incorrect predictions made in each label as follows:\n",
    "```\n",
    "              +---------------------------------------------+\n",
    "              |                Predicted label              |\n",
    "              +----------------------+----------------------+\n",
    "              |          (+1)        |         (-1)         |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "| True  |(+1) | # of true positives  | # of false negatives |\n",
    "| label +-----+----------------------+----------------------+\n",
    "|       |(-1) | # of false positives | # of true negatives  |\n",
    "+-------+-----+----------------------+----------------------+\n",
    "```\n",
    "To print out the confusion matrix for a classifier, use `metric='confusion_matrix'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target_label | predicted_label | count \n",
      "--------------+-----------------+-------\n",
      "     -1       |       -1        |  3788\n",
      "     -1       |        1        |  1453\n",
      "      1       |       -1        |   804\n",
      "      1       |        1        | 27291\n"
     ]
    }
   ],
   "source": [
    "# confusion_matrix = model.evaluate(test_data, metric='confusion_matrix')['confusion_matrix']\n",
    "# confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_true=test_data['sentiment'].to_numpy(),\n",
    "                        y_pred=model.predict(test_matrix),\n",
    "                        labels=model.classes_)    # use the same order of class as the LR model.\n",
    "print ' target_label | predicted_label | count '\n",
    "print '--------------+-----------------+-------'\n",
    "# Print out the confusion matrix.\n",
    "# NOTE: Your tool may arrange entries in a different order. Consult appropriate manuals.\n",
    "for i, target_label in enumerate(model.classes_):\n",
    "    for j, predicted_label in enumerate(model.classes_):\n",
    "        print '{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: How many predicted values in the **test set** are **false positives**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1453"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1453"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the cost of mistakes\n",
    "\n",
    "\n",
    "Put yourself in the shoes of a manufacturer that sells a baby product on Amazon.com and you want to monitor your product's reviews in order to respond to complaints.  Even a few negative reviews may generate a lot of bad publicity about the product. So you don't want to miss any reviews with negative sentiments --- you'd rather put up with false alarms about potentially negative reviews instead of missing negative reviews entirely. In other words, **false positives cost more than false negatives**. (It may be the other way around for other scenarios, but let's stick with the manufacturer's scenario for now.)\n",
    "\n",
    "Suppose you know the costs involved in each kind of mistake: \n",
    "1. \\$100 for each false positive.\n",
    "2. \\$1 for each false negative.\n",
    "3. Correctly classified reviews incur no cost.\n",
    "\n",
    "**Quiz Question**: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the **test set**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146104"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 * 1453) + (1 * 804)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may not have exact dollar amounts for each kind of mistake. Instead, you may simply prefer to reduce the percentage of false positives to be less than, say, 3.5% of all positive predictions. This is where **precision** comes in:\n",
    "\n",
    "$$\n",
    "[\\text{precision}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all data points with positive predictions]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false positives}]}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to keep the percentage of false positives below 3.5% of positive predictions, we must raise the precision to 96.5% or higher. \n",
    "\n",
    "**First**, let us compute the precision of the logistic regression classifier on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on test data: 0.949450320067\n"
     ]
    }
   ],
   "source": [
    "# precision = model.evaluate(test_data, metric='precision')['precision']\n",
    "# print \"Precision on test data: %s\" % precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=model.predict(test_matrix))\n",
    "print \"Precision on test data: %s\" % precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Out of all reviews in the **test set** that are predicted to be positive, what fraction of them are **false positives**? (Round to the second decimal place e.g. 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05054967993299997"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - 0.949450320067"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Based on what we learned in lecture, if we wanted to reduce this fraction of false positives to be below 3.5%, we would: (see the quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A complementary metric is **recall**, which measures the ratio between the number of true positives and that of (ground-truth) positive reviews:\n",
    "\n",
    "$$\n",
    "[\\text{recall}] = \\frac{[\\text{# positive data points with positive predicitions}]}{\\text{[# all positive data points]}} = \\frac{[\\text{# true positives}]}{[\\text{# true positives}] + [\\text{# false negatives}]}\n",
    "$$\n",
    "\n",
    "Let us compute the recall on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on test data: 0.971382808329\n"
     ]
    }
   ],
   "source": [
    "# recall = model.evaluate(test_data, metric='recall')['recall']\n",
    "# print \"Recall on test data: %s\" % recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_true=test_data['sentiment'].to_numpy(),\n",
    "                      y_pred=model.predict(test_matrix))\n",
    "print \"Recall on test data: %s\" % recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall for all 1 on test data: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "recall = recall_score(y_true=test_data['sentiment'].to_numpy(),\n",
    "                      y_pred=np.ones(len(test_data)))\n",
    "print \"Recall for all 1 on test data: %s\" % recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: What fraction of the positive reviews in the **test_set** were correctly predicted as positive by the classifier?\n",
    "\n",
    "**Quiz Question**: What is the recall value for a classifier that predicts **+1** for all data points in the **test_data**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Precision-recall tradeoff\n",
    "\n",
    "In this part, we will explore the trade-off between precision and recall discussed in the lecture.  We first examine what happens when we use a different threshold value for making class predictions.  We then explore a range of threshold values and plot the associated precision-recall curve.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the threshold\n",
    "\n",
    "False positives are costly in our example, so we may want to be more conservative about making positive predictions. To achieve this, instead of thresholding class probabilities at 0.5, we can choose a higher threshold. \n",
    "\n",
    "Write a function called `apply_threshold` that accepts two things\n",
    "* `probabilities` (an SArray of probability values)\n",
    "* `threshold` (a float between 0 and 1).\n",
    "\n",
    "The function should return an array, where each element is set to +1 or -1 depending whether the corresponding probability exceeds `threshold`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def apply_threshold(probabilities, threshold):\n",
    "    ### YOUR CODE GOES HERE\n",
    "    # +1 if >= threshold and -1 otherwise.\n",
    "    return [1 if x > threshold else -1 for x in probabilities]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction with `output_type='probability'` to get the list of probability values. Then use thresholds set at 0.5 (default) and 0.9 to make predictions from these probability values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities = model.predict(test_data, output_type='probability')\n",
    "probabilities = model.predict_proba(test_matrix)[:,1]\n",
    "predictions_with_default_threshold = apply_threshold(probabilities, 0.5)\n",
    "predictions_with_high_threshold = apply_threshold(probabilities, 0.9)\n",
    "predictions_with_default_threshold[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predicted reviews (threshold = 0.5): 28744\n"
     ]
    }
   ],
   "source": [
    "print \"Number of positive predicted reviews (threshold = 0.5): %s\" % \\\n",
    "                        (sum([x for x in predictions_with_default_threshold if x == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive predicted reviews (threshold = 0.9): 25070\n"
     ]
    }
   ],
   "source": [
    "print \"Number of positive predicted reviews (threshold = 0.9): %s\" % \\\n",
    "                        (sum([x for x in predictions_with_high_threshold if x == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: What happens to the number of positive predicted reviews as the threshold increased from 0.5 to 0.9?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the associated precision and recall as the threshold varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing the probability threshold, it is possible to influence precision and recall. We can explore this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Threshold = 0.5\n",
    "# precision_with_default_threshold = graphlab.evaluation.precision(test_data['sentiment'],\n",
    "#                                         predictions_with_default_threshold)\n",
    "precision_with_default_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions_with_default_threshold)\n",
    "\n",
    "# recall_with_default_threshold = graphlab.evaluation.recall(test_data['sentiment'],\n",
    "#                                         predictions_with_default_threshold)\n",
    "\n",
    "recall_with_default_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions_with_default_threshold)\n",
    "# Threshold = 0.9\n",
    "# precision_with_high_threshold = graphlab.evaluation.precision(test_data['sentiment'],\n",
    "#                                         predictions_with_high_threshold)\n",
    "precision_with_high_threshold = precision_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions_with_high_threshold)\n",
    "# recall_with_high_threshold = graphlab.evaluation.recall(test_data['sentiment'],\n",
    "#                                         predictions_with_high_threshold)\n",
    "recall_with_high_threshold = recall_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions_with_high_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (threshold = 0.5): 0.949450320067\n",
      "Recall (threshold = 0.5)   : 0.971382808329\n"
     ]
    }
   ],
   "source": [
    "print \"Precision (threshold = 0.5): %s\" % precision_with_default_threshold\n",
    "print \"Recall (threshold = 0.5)   : %s\" % recall_with_default_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (threshold = 0.9): 0.981491822896\n",
      "Recall (threshold = 0.9)   : 0.875814201815\n"
     ]
    }
   ],
   "source": [
    "print \"Precision (threshold = 0.9): %s\" % precision_with_high_threshold\n",
    "print \"Recall (threshold = 0.9)   : %s\" % recall_with_high_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question (variant 1)**: Does the **precision** increase with a higher threshold?\n",
    "\n",
    "**Quiz Question (variant 2)**: Does the **recall** increase with a higher threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-recall curve\n",
    "\n",
    "Now, we will explore various different values of tresholds, compute the precision and recall scores, and then plot the precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5         0.50505051  0.51010101  0.51515152  0.52020202  0.52525253\n",
      "  0.53030303  0.53535354  0.54040404  0.54545455  0.55050505  0.55555556\n",
      "  0.56060606  0.56565657  0.57070707  0.57575758  0.58080808  0.58585859\n",
      "  0.59090909  0.5959596   0.6010101   0.60606061  0.61111111  0.61616162\n",
      "  0.62121212  0.62626263  0.63131313  0.63636364  0.64141414  0.64646465\n",
      "  0.65151515  0.65656566  0.66161616  0.66666667  0.67171717  0.67676768\n",
      "  0.68181818  0.68686869  0.69191919  0.6969697   0.7020202   0.70707071\n",
      "  0.71212121  0.71717172  0.72222222  0.72727273  0.73232323  0.73737374\n",
      "  0.74242424  0.74747475  0.75252525  0.75757576  0.76262626  0.76767677\n",
      "  0.77272727  0.77777778  0.78282828  0.78787879  0.79292929  0.7979798\n",
      "  0.8030303   0.80808081  0.81313131  0.81818182  0.82323232  0.82828283\n",
      "  0.83333333  0.83838384  0.84343434  0.84848485  0.85353535  0.85858586\n",
      "  0.86363636  0.86868687  0.87373737  0.87878788  0.88383838  0.88888889\n",
      "  0.89393939  0.8989899   0.9040404   0.90909091  0.91414141  0.91919192\n",
      "  0.92424242  0.92929293  0.93434343  0.93939394  0.94444444  0.94949495\n",
      "  0.95454545  0.95959596  0.96464646  0.96969697  0.97474747  0.97979798\n",
      "  0.98484848  0.98989899  0.99494949  1.        ]\n"
     ]
    }
   ],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)\n",
    "print threshold_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the values of threshold, we compute the precision and recall scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Threshold 0.5 Precision 0.949450320067 Recall 0.971382808329\n",
      "Metrics Threshold 0.505050505051 Precision 0.950020897186 Recall 0.970884499021\n",
      "Metrics Threshold 0.510101010101 Precision 0.950360790602 Recall 0.970386189713\n",
      "Metrics Threshold 0.515151515152 Precision 0.951005025126 Recall 0.969994660972\n",
      "Metrics Threshold 0.520202020202 Precision 0.951388161341 Recall 0.969674319274\n",
      "Metrics Threshold 0.525252525253 Precision 0.951768488746 Recall 0.969282790532\n",
      "Metrics Threshold 0.530303030303 Precision 0.95218440659 Recall 0.968926855312\n",
      "Metrics Threshold 0.535353535354 Precision 0.952534304116 Recall 0.968570920093\n",
      "Metrics Threshold 0.540404040404 Precision 0.952884716433 Recall 0.968214984873\n",
      "Metrics Threshold 0.545454545455 Precision 0.953202229467 Recall 0.967859049653\n",
      "Metrics Threshold 0.550505050505 Precision 0.953789473684 Recall 0.967538707955\n",
      "Metrics Threshold 0.555555555556 Precision 0.954009268361 Recall 0.967218366257\n",
      "Metrics Threshold 0.560606060606 Precision 0.954227702252 Recall 0.966862431038\n",
      "Metrics Threshold 0.565656565657 Precision 0.954745244207 Recall 0.966435308774\n",
      "Metrics Threshold 0.570707070707 Precision 0.954921349896 Recall 0.965865812422\n",
      "Metrics Threshold 0.575757575758 Precision 0.955096147073 Recall 0.965260722548\n",
      "Metrics Threshold 0.580808080808 Precision 0.955508549268 Recall 0.964691226197\n",
      "Metrics Threshold 0.585858585859 Precision 0.955860560299 Recall 0.964264103933\n",
      "Metrics Threshold 0.590909090909 Precision 0.956136323503 Recall 0.963623420537\n",
      "Metrics Threshold 0.59595959596 Precision 0.956587831866 Recall 0.96312511123\n",
      "Metrics Threshold 0.60101010101 Precision 0.956904787178 Recall 0.962626801922\n",
      "Metrics Threshold 0.606060606061 Precision 0.957320960544 Recall 0.96205730557\n",
      "Metrics Threshold 0.611111111111 Precision 0.957649643832 Recall 0.961808150917\n",
      "Metrics Threshold 0.616161616162 Precision 0.957792438107 Recall 0.961167467521\n",
      "Metrics Threshold 0.621212121212 Precision 0.958281555122 Recall 0.960669158213\n",
      "Metrics Threshold 0.626262626263 Precision 0.95887245841 Recall 0.960135255384\n",
      "Metrics Threshold 0.631313131313 Precision 0.959403686046 Recall 0.959779320164\n",
      "Metrics Threshold 0.636363636364 Precision 0.959780556446 Recall 0.958960669158\n",
      "Metrics Threshold 0.641414141414 Precision 0.960303873315 Recall 0.958355579285\n",
      "Metrics Threshold 0.646464646465 Precision 0.960732517046 Recall 0.957928457021\n",
      "Metrics Threshold 0.651515151515 Precision 0.961076560154 Recall 0.957074212493\n",
      "Metrics Threshold 0.656565656566 Precision 0.961332093289 Recall 0.956575903186\n",
      "Metrics Threshold 0.661616161616 Precision 0.961826320501 Recall 0.956006406834\n",
      "Metrics Threshold 0.666666666667 Precision 0.96225265271 Recall 0.955436910482\n",
      "Metrics Threshold 0.671717171717 Precision 0.962751641727 Recall 0.954938601175\n",
      "Metrics Threshold 0.676767676768 Precision 0.962864530958 Recall 0.954262324257\n",
      "Metrics Threshold 0.681818181818 Precision 0.96324402086 Recall 0.953301299164\n",
      "Metrics Threshold 0.686868686869 Precision 0.963564356436 Recall 0.952589428724\n",
      "Metrics Threshold 0.691919191919 Precision 0.964168559172 Recall 0.952019932372\n",
      "Metrics Threshold 0.69696969697 Precision 0.964450700159 Recall 0.951165687845\n",
      "Metrics Threshold 0.70202020202 Precision 0.964733514002 Recall 0.950311443317\n",
      "Metrics Threshold 0.707070707071 Precision 0.96515162481 Recall 0.949314824702\n",
      "Metrics Threshold 0.712121212121 Precision 0.965304939881 Recall 0.948709734828\n",
      "Metrics Threshold 0.717171717172 Precision 0.965478478442 Recall 0.947677522691\n",
      "Metrics Threshold 0.722222222222 Precision 0.965766354462 Recall 0.946894465207\n",
      "Metrics Threshold 0.727272727273 Precision 0.966145454545 Recall 0.94568428546\n",
      "Metrics Threshold 0.732323232323 Precision 0.96643489024 Recall 0.944901227977\n",
      "Metrics Threshold 0.737373737374 Precision 0.967034970645 Recall 0.943904609361\n",
      "Metrics Threshold 0.742424242424 Precision 0.967594622242 Recall 0.942694429614\n",
      "Metrics Threshold 0.747474747475 Precision 0.967836364302 Recall 0.941448656345\n",
      "Metrics Threshold 0.752525252525 Precision 0.968164999817 Recall 0.940665598861\n",
      "Metrics Threshold 0.757575757576 Precision 0.968629609246 Recall 0.939668980246\n",
      "Metrics Threshold 0.762626262626 Precision 0.969203630885 Recall 0.938707955152\n",
      "Metrics Threshold 0.767676767677 Precision 0.96956089661 Recall 0.937604555971\n",
      "Metrics Threshold 0.772727272727 Precision 0.970306160089 Recall 0.936287595658\n",
      "Metrics Threshold 0.777777777778 Precision 0.970701248799 Recall 0.935148602954\n",
      "Metrics Threshold 0.782828282828 Precision 0.970951746596 Recall 0.933938423207\n",
      "Metrics Threshold 0.787878787879 Precision 0.97148577997 Recall 0.93255027585\n",
      "Metrics Threshold 0.792929292929 Precision 0.971919919771 Recall 0.931375689624\n",
      "Metrics Threshold 0.79797979798 Precision 0.972315248939 Recall 0.930058729311\n",
      "Metrics Threshold 0.80303030303 Precision 0.973541995046 Recall 0.923331553657\n",
      "Metrics Threshold 0.808080808081 Precision 0.97389112524 Recall 0.92140950347\n",
      "Metrics Threshold 0.813131313131 Precision 0.974215922799 Recall 0.919878982025\n",
      "Metrics Threshold 0.818181818182 Precision 0.974395770393 Recall 0.918384054102\n",
      "Metrics Threshold 0.823232323232 Precision 0.974665808308 Recall 0.916106068695\n",
      "Metrics Threshold 0.828282828283 Precision 0.975356925881 Recall 0.914290799075\n",
      "Metrics Threshold 0.833333333333 Precision 0.975827020442 Recall 0.91240434241\n",
      "Metrics Threshold 0.838383838384 Precision 0.976234073396 Recall 0.910873820965\n",
      "Metrics Threshold 0.843434343434 Precision 0.976496216464 Recall 0.909450080085\n",
      "Metrics Threshold 0.848484848485 Precision 0.976671135798 Recall 0.907492436377\n",
      "Metrics Threshold 0.853535353535 Precision 0.97717140661 Recall 0.905000889838\n",
      "Metrics Threshold 0.858585858586 Precision 0.977482166956 Recall 0.90233137569\n",
      "Metrics Threshold 0.863636363636 Precision 0.978058124686 Recall 0.899590674497\n",
      "Metrics Threshold 0.868686868687 Precision 0.978379847067 Recall 0.897170315003\n",
      "Metrics Threshold 0.873737373737 Precision 0.978972781434 Recall 0.894856736074\n",
      "Metrics Threshold 0.878787878788 Precision 0.979636505765 Recall 0.892116034882\n",
      "Metrics Threshold 0.883838383838 Precision 0.97992851251 Recall 0.887987186332\n",
      "Metrics Threshold 0.888888888889 Precision 0.980272243046 Recall 0.884321053568\n",
      "Metrics Threshold 0.893939393939 Precision 0.980886668253 Recall 0.880441359673\n",
      "Metrics Threshold 0.89898989899 Precision 0.981471150781 Recall 0.876704039865\n",
      "Metrics Threshold 0.90404040404 Precision 0.981856049986 Recall 0.872539597793\n",
      "Metrics Threshold 0.909090909091 Precision 0.982407407407 Recall 0.868588716854\n",
      "Metrics Threshold 0.914141414141 Precision 0.982907367654 Recall 0.863747997864\n",
      "Metrics Threshold 0.919191919192 Precision 0.983274863343 Recall 0.857946253782\n",
      "Metrics Threshold 0.924242424242 Precision 0.983971724478 Recall 0.852180103221\n",
      "Metrics Threshold 0.929292929293 Precision 0.984292108753 Recall 0.845310553479\n",
      "Metrics Threshold 0.934343434343 Precision 0.98505901063 Recall 0.83776472682\n",
      "Metrics Threshold 0.939393939394 Precision 0.985304674634 Recall 0.830503648336\n",
      "Metrics Threshold 0.944444444444 Precision 0.986036978522 Recall 0.821925609539\n",
      "Metrics Threshold 0.949494949495 Precision 0.986732356627 Recall 0.812671293825\n",
      "Metrics Threshold 0.954545454545 Precision 0.987241878206 Recall 0.801494927923\n",
      "Metrics Threshold 0.959595959596 Precision 0.987619132449 Recall 0.789321943406\n",
      "Metrics Threshold 0.964646464646 Precision 0.988224061108 Recall 0.773625200214\n",
      "Metrics Threshold 0.969696969697 Precision 0.988797992005 Recall 0.757180993059\n",
      "Metrics Threshold 0.974747474747 Precision 0.989426849105 Recall 0.736109628048\n",
      "Metrics Threshold 0.979797979798 Precision 0.990355935574 Recall 0.709094144866\n",
      "Metrics Threshold 0.984848484848 Precision 0.99080315619 Recall 0.674888770244\n",
      "Metrics Threshold 0.989898989899 Precision 0.992177757624 Recall 0.62302900872\n",
      "Metrics Threshold 0.994949494949 Precision 0.992693457323 Recall 0.531945185976\n",
      "Metrics Threshold 1.0 Precision 0.0 Recall 0.0\n"
     ]
    }
   ],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "# probabilities = model.predict(test_data, output_type='probability')\n",
    "probabilities = model.predict_proba(test_matrix)[:,1]\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "    \n",
    "#     precision = graphlab.evaluation.precision(test_data['sentiment'], predictions)\n",
    "    precision = precision_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions)\n",
    "#     recall = graphlab.evaluation.recall(test_data['sentiment'], predictions)\n",
    "    recall = recall_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions)\n",
    "    \n",
    "    print 'Metrics Threshold %s Precision %s Recall %s' % (threshold, precision, recall)\n",
    "    \n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the precision-recall curve to visualize the precision-recall tradeoff as we vary the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFgCAYAAAAYQGiBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngQJWREQkBgpYERGRGChgRUREYvD/ATopV2iHoVWkAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x138ff30d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_pr_curve(precision, recall, title):\n",
    "    plt.rcParams['figure.figsize'] = 7, 5\n",
    "    plt.locator_params(axis = 'x', nbins = 5)\n",
    "    plt.plot(precision, recall, 'b-', linewidth=4.0, color = '#B0017F')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlim(0.94, 1.0)\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    \n",
    "plot_pr_curve(precision_all, recall_all, 'Precision recall curve (all)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.707"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Using `threshold` = 0.98, how many **false negatives** do we get on the **test_data**? (**Hint**: You may use the `graphlab.evaluation.confusion_matrix` function implemented in GraphLab Create.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target_label | predicted_label | count \n",
      "--------------+-----------------+-------\n",
      "     -1       |       -1        |  5047\n",
      "     -1       |        1        |   194\n",
      "      1       |       -1        |  8208\n",
      "      1       |        1        | 19887\n"
     ]
    }
   ],
   "source": [
    "cmat = confusion_matrix(y_true=test_data['sentiment'].to_numpy(),\n",
    "                        y_pred=apply_threshold(probabilities, 0.98),\n",
    "                        labels=model.classes_)    # use the same order of class as the LR model.\n",
    "print ' target_label | predicted_label | count '\n",
    "print '--------------+-----------------+-------'\n",
    "# Print out the confusion matrix.\n",
    "# NOTE: Your tool may arrange entries in a different order. Consult appropriate manuals.\n",
    "for i, target_label in enumerate(model.classes_):\n",
    "    for j, predicted_label in enumerate(model.classes_):\n",
    "        print '{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is the number of false negatives (i.e the number of reviews to look at when not needed) that we have to deal with using this classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating specific search terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we looked at the number of false positives for the **entire test set**. In this section, let's select reviews using a specific search term and optimize the precision on these reviews only. After all, a manufacturer would be interested in tuning the false positive rate just for their products (the reviews they want to read) rather than that of the entire set of products on Amazon.\n",
    "\n",
    "## Precision-Recall on all baby related items\n",
    "\n",
    "From the **test set**, select all the reviews for all products with the word 'baby' in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "baby_reviews =  test_data[test_data['name'].apply(lambda x: 'baby' in x.lower())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's predict the probability of classifying these reviews as positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# probabilities = model.predict(baby_reviews, output_type='probability')\n",
    "baby_matrix = vectorizer.transform(baby_reviews['review_clean'])\n",
    "probabilities = model.predict_proba(baby_matrix)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the precision-recall curve for the **baby_reviews** dataset.\n",
    "\n",
    "**First**, let's consider the following `threshold_values` ranging from 0.5 to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold_values = np.linspace(0.5, 1, num=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Second**, as we did above, let's compute precision and recall for each value in `threshold_values` on the **baby_reviews** dataset.  Complete the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics Threshold 0.5 Precision 0.946657183499 Recall 0.967824031994\n",
      "Metrics Threshold 0.505050505051 Precision 0.947134211463 Recall 0.967278676604\n",
      "Metrics Threshold 0.510101010101 Precision 0.94760292283 Recall 0.966551536084\n",
      "Metrics Threshold 0.515151515152 Precision 0.948420489024 Recall 0.966006180694\n",
      "Metrics Threshold 0.520202020202 Precision 0.948919449902 Recall 0.965824395564\n",
      "Metrics Threshold 0.525252525253 Precision 0.949231319271 Recall 0.965279040175\n",
      "Metrics Threshold 0.530303030303 Precision 0.949704671559 Recall 0.964551899655\n",
      "Metrics Threshold 0.535353535354 Precision 0.949856733524 Recall 0.964188329395\n",
      "Metrics Threshold 0.540404040404 Precision 0.950519899606 Recall 0.963824759135\n",
      "Metrics Threshold 0.545454545455 Precision 0.951031390135 Recall 0.963824759135\n",
      "Metrics Threshold 0.550505050505 Precision 0.951867816092 Recall 0.963461188875\n",
      "Metrics Threshold 0.555555555556 Precision 0.952201257862 Recall 0.963279403745\n",
      "Metrics Threshold 0.560606060606 Precision 0.952535059331 Recall 0.963097618615\n",
      "Metrics Threshold 0.565656565657 Precision 0.952852258413 Recall 0.962552263225\n",
      "Metrics Threshold 0.570707070707 Precision 0.953350144092 Recall 0.962188692965\n",
      "Metrics Threshold 0.575757575758 Precision 0.953660295709 Recall 0.961461552445\n",
      "Metrics Threshold 0.580808080808 Precision 0.954135066811 Recall 0.960552626795\n",
      "Metrics Threshold 0.585858585859 Precision 0.954274353877 Recall 0.959825486275\n",
      "Metrics Threshold 0.590909090909 Precision 0.954438618695 Recall 0.959643701145\n",
      "Metrics Threshold 0.59595959596 Precision 0.954915806627 Recall 0.958734775495\n",
      "Metrics Threshold 0.60101010101 Precision 0.954891304348 Recall 0.958189420105\n",
      "Metrics Threshold 0.606060606061 Precision 0.955039883974 Recall 0.957644064716\n",
      "Metrics Threshold 0.611111111111 Precision 0.95590636908 Recall 0.957644064716\n",
      "Metrics Threshold 0.616161616162 Precision 0.956197746274 Recall 0.956371568806\n",
      "Metrics Threshold 0.621212121212 Precision 0.956513828239 Recall 0.955644428286\n",
      "Metrics Threshold 0.626262626263 Precision 0.957885141294 Recall 0.955099072896\n",
      "Metrics Threshold 0.631313131313 Precision 0.958211678832 Recall 0.954553717506\n",
      "Metrics Threshold 0.636363636364 Precision 0.958363769175 Recall 0.954008362116\n",
      "Metrics Threshold 0.641414141414 Precision 0.959026888604 Recall 0.953099436466\n",
      "Metrics Threshold 0.646464646465 Precision 0.959523809524 Recall 0.952372295946\n",
      "Metrics Threshold 0.651515151515 Precision 0.96016886931 Recall 0.950918014906\n",
      "Metrics Threshold 0.656565656566 Precision 0.960146923783 Recall 0.950372659516\n",
      "Metrics Threshold 0.661616161616 Precision 0.960470674756 Recall 0.949645518997\n",
      "Metrics Threshold 0.666666666667 Precision 0.960795140806 Recall 0.948918378477\n",
      "Metrics Threshold 0.671717171717 Precision 0.96130458817 Recall 0.948373023087\n",
      "Metrics Threshold 0.676767676768 Precision 0.961268904463 Recall 0.947464097437\n",
      "Metrics Threshold 0.681818181818 Precision 0.961218836565 Recall 0.946191601527\n",
      "Metrics Threshold 0.686868686869 Precision 0.961901239134 Recall 0.945464461007\n",
      "Metrics Threshold 0.691919191919 Precision 0.962763986662 Recall 0.944737320487\n",
      "Metrics Threshold 0.69696969697 Precision 0.963450834879 Recall 0.944010179967\n",
      "Metrics Threshold 0.70202020202 Precision 0.963768115942 Recall 0.942919469187\n",
      "Metrics Threshold 0.707070707071 Precision 0.964265773311 Recall 0.941828758408\n",
      "Metrics Threshold 0.712121212121 Precision 0.96425912137 Recall 0.941646973278\n",
      "Metrics Threshold 0.717171717172 Precision 0.964245810056 Recall 0.941283403018\n",
      "Metrics Threshold 0.722222222222 Precision 0.964405516213 Recall 0.940738047628\n",
      "Metrics Threshold 0.727272727273 Precision 0.964552238806 Recall 0.939829121978\n",
      "Metrics Threshold 0.732323232323 Precision 0.964899178491 Recall 0.939465551718\n",
      "Metrics Threshold 0.737373737374 Precision 0.966647929548 Recall 0.937829485548\n",
      "Metrics Threshold 0.742424242424 Precision 0.96753003003 Recall 0.937102345028\n",
      "Metrics Threshold 0.747474747475 Precision 0.967857142857 Recall 0.936011634248\n",
      "Metrics Threshold 0.752525252525 Precision 0.968185240964 Recall 0.934920923468\n",
      "Metrics Threshold 0.757575757576 Precision 0.968532127379 Recall 0.934375568079\n",
      "Metrics Threshold 0.762626262626 Precision 0.96885617214 Recall 0.933103072169\n",
      "Metrics Threshold 0.767676767677 Precision 0.968986384266 Recall 0.931467005999\n",
      "Metrics Threshold 0.772727272727 Precision 0.969673995451 Recall 0.930012724959\n",
      "Metrics Threshold 0.777777777778 Precision 0.970001898614 Recall 0.928740229049\n",
      "Metrics Threshold 0.782828282828 Precision 0.970135058018 Recall 0.927104162879\n",
      "Metrics Threshold 0.787878787879 Precision 0.970274390244 Recall 0.92564988184\n",
      "Metrics Threshold 0.792929292929 Precision 0.970784800458 Recall 0.9241956008\n",
      "Metrics Threshold 0.79797979798 Precision 0.971291866029 Recall 0.92255953463\n",
      "Metrics Threshold 0.80303030303 Precision 0.972952086553 Recall 0.915469914561\n",
      "Metrics Threshold 0.808080808081 Precision 0.973093302362 Recall 0.913833848391\n",
      "Metrics Threshold 0.813131313131 Precision 0.973250629967 Recall 0.912743137611\n",
      "Metrics Threshold 0.818181818182 Precision 0.973177842566 Recall 0.910198145792\n",
      "Metrics Threshold 0.823232323232 Precision 0.973489278752 Recall 0.907834939102\n",
      "Metrics Threshold 0.828282828283 Precision 0.973817897616 Recall 0.906017087802\n",
      "Metrics Threshold 0.833333333333 Precision 0.974148061105 Recall 0.904199236502\n",
      "Metrics Threshold 0.838383838384 Precision 0.974484789009 Recall 0.902563170333\n",
      "Metrics Threshold 0.843434343434 Precision 0.975024582104 Recall 0.901290674423\n",
      "Metrics Threshold 0.848484848485 Precision 0.974975369458 Recall 0.899472823123\n",
      "Metrics Threshold 0.853535353535 Precision 0.975658024936 Recall 0.896200690783\n",
      "Metrics Threshold 0.858585858586 Precision 0.975977764542 Recall 0.893655698964\n",
      "Metrics Threshold 0.863636363636 Precision 0.976656025539 Recall 0.889838211234\n",
      "Metrics Threshold 0.868686868687 Precision 0.976795359072 Recall 0.887656789675\n",
      "Metrics Threshold 0.873737373737 Precision 0.977117623444 Recall 0.884930012725\n",
      "Metrics Threshold 0.878787878788 Precision 0.978221415608 Recall 0.881839665515\n",
      "Metrics Threshold 0.883838383838 Precision 0.978913219789 Recall 0.877658607526\n",
      "Metrics Threshold 0.888888888889 Precision 0.979048006509 Recall 0.874931830576\n",
      "Metrics Threshold 0.893939393939 Precision 0.979779411765 Recall 0.872023268497\n",
      "Metrics Threshold 0.89898989899 Precision 0.980275323608 Recall 0.867296855117\n",
      "Metrics Threshold 0.90404040404 Precision 0.980606560759 Recall 0.864024722778\n",
      "Metrics Threshold 0.909090909091 Precision 0.980928689884 Recall 0.860207235048\n",
      "Metrics Threshold 0.914141414141 Precision 0.981257809246 Recall 0.856571532449\n",
      "Metrics Threshold 0.919191919192 Precision 0.981734201134 Recall 0.850027267769\n",
      "Metrics Threshold 0.924242424242 Precision 0.982667512154 Recall 0.84511906926\n",
      "Metrics Threshold 0.929292929293 Precision 0.982924226254 Recall 0.837120523541\n",
      "Metrics Threshold 0.934343434343 Precision 0.984442523768 Recall 0.828213052172\n",
      "Metrics Threshold 0.939393939394 Precision 0.984960767219 Recall 0.821487002363\n",
      "Metrics Threshold 0.944444444444 Precision 0.985689123734 Recall 0.813852026904\n",
      "Metrics Threshold 0.949494949495 Precision 0.986411227445 Recall 0.804944555535\n",
      "Metrics Threshold 0.954545454545 Precision 0.98752551599 Recall 0.791492455917\n",
      "Metrics Threshold 0.959595959596 Precision 0.988026709648 Recall 0.780039992729\n",
      "Metrics Threshold 0.964646464646 Precision 0.988243592758 Recall 0.764042901291\n",
      "Metrics Threshold 0.969696969697 Precision 0.9888996139 Recall 0.744955462643\n",
      "Metrics Threshold 0.974747474747 Precision 0.989562624254 Recall 0.723868387566\n",
      "Metrics Threshold 0.979797979798 Precision 0.990157990158 Recall 0.6949645519\n",
      "Metrics Threshold 0.984848484848 Precision 0.990447598253 Recall 0.659698236684\n",
      "Metrics Threshold 0.989898989899 Precision 0.991679049034 Recall 0.606616978731\n",
      "Metrics Threshold 0.994949494949 Precision 0.991681109185 Recall 0.520087256862\n",
      "Metrics Threshold 1.0 Precision 0.0 Recall 0.0\n"
     ]
    }
   ],
   "source": [
    "precision_all = []\n",
    "recall_all = []\n",
    "\n",
    "for threshold in threshold_values:\n",
    "    \n",
    "    # Make predictions. Use the `apply_threshold` function     \n",
    "    predictions = apply_threshold(probabilities, threshold)\n",
    "    \n",
    "#     precision = graphlab.evaluation.precision(test_data['sentiment'], predictions)\n",
    "    precision = precision_score(y_true=baby_reviews['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions)\n",
    "#     recall = graphlab.evaluation.recall(test_data['sentiment'], predictions)\n",
    "    recall = recall_score(y_true=baby_reviews['sentiment'].to_numpy(), \n",
    "                            y_pred=predictions)\n",
    "    \n",
    "    print 'Metrics Threshold %s Precision %s Recall %s' % (threshold, precision, recall)\n",
    "    \n",
    "    precision_all.append(precision)\n",
    "    recall_all.append(recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Among all the threshold values tried, what is the **smallest** threshold value that achieves a precision of 96.5% or better for the reviews of data in **baby_reviews**? Round your answer to 3 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "0.737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Is this threshold value smaller or larger than the threshold used for the entire dataset to achieve the same specified precision of 96.5%?\n",
    "\n",
    "**Finally**, let's plot the precision recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdgAAAFgCAYAAAAYQGiBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAwWsiIhIDBSwIiIiMVDAioiIxOD/A70hqlsy8WM4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1235acd50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_pr_curve(precision_all, recall_all, \"Precision-Recall (Baby)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
